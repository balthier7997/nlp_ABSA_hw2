{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp2021-hw2': conda)"
  },
  "interpreter": {
   "hash": "42797de3e02398c5942c0db88a25dee72f89b0c613793ce0cb656faf0baecb54"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install -r requirements.txt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "from dataset_class import TermIdentificationDataset\n",
    "\n",
    "from model_class   import TermIdentificationBaseModule, TermIdentificationModel\n",
    "from model_class   import PreTrainedEmbeddingLayer\n",
    "\n",
    "TRAIN_FILE_R = 'data/restaurants_train.json'\n",
    "DEVEL_FILE_R = '../../data/restaurants_dev.json'\n",
    "TRAIN_FILE_L = 'data/laptops_train.json'\n",
    "DEVEL_FILE_L = 'data/laptops_dev.json'\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# dataset loader\n",
    "train_dataset = TermIdentificationDataset(DEVEL_FILE_R, size=3)\n",
    "\n",
    "# hyperparameters\n",
    "hparams = { 'vocab'           : train_dataset.vocab, \n",
    "            'vocab_size'      : len(train_dataset.vocab),\n",
    "            'embedding_dim'   : 100, \n",
    "            'lstm_hidden_dim' : 128,\n",
    "            'lstm_bidirect'   : False, \n",
    "            'lstm_layers'     : 1, \n",
    "            'num_classes'     : 3, \n",
    "            'dropout'         : 0.0, \n",
    "            'device'          : device}\n",
    "\n",
    "\n",
    "# retrieve embeddings\n",
    "emb = PreTrainedEmbeddingLayer(hparams)   # loads glove pre trained embeddings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from model_class import HParams\n",
    "train_dataset = TermIdentificationDataset(DEVEL_FILE_R, size=3)\n",
    "hparams = HParams(train_dataset.vocab)\n",
    "basemodel = TermIdentificationBaseModule(hparams, emb.get_embeddings()).cuda()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "elem = train_dataset[0]\n",
    "x = elem['idxs_vector']\n",
    "batched_x = x.unsqueeze(1)\n",
    "print(batched_x)\n",
    "batched_logits = basemodel.forward(batched_x)\n",
    "logits = batched_logits.squeeze()\n",
    "print(logits, '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.argmax(logits, -1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT TESTS\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaTokenizer\n",
    "tokenizer_bert    = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "sentence = \"Hello I'm Luca and I have played a lot\"\n",
    "tokenize_out_bert    = tokenizer_bert.tokenize(sentence)   \n",
    "tokenize_out_roberta = tokenizer_roberta.tokenize(sentence)\n",
    "\n",
    "encode_out_bert      = tokenizer_bert.encode(sentence)\n",
    "encode_out_roberta   = tokenizer_roberta.encode(sentence)\n",
    "\n",
    "tokenizer_out_bert    = tokenizer_bert(sentence, return_tensors='pt')\n",
    "tokenizer_out_roberta = tokenizer_roberta(sentence, return_tensors='pt')\n",
    "\n",
    "print(\"BERT   \", tokenize_out_bert)\n",
    "print(\"ROBERTA\", tokenize_out_roberta)\n",
    "print(\"BERT   \", encode_out_bert)\n",
    "print(\"ROBERTA\", encode_out_roberta)\n",
    "print(\"BERT   \", tokenizer_out_bert)\n",
    "print(\"ROBERTA\", tokenizer_out_roberta)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BERT    ['Hello', 'I', \"'\", 'm', 'Luca', 'and', 'I', 'have', 'played', 'a', 'lot']\n",
      "ROBERTA ['Hello', 'ĠI', \"'m\", 'ĠLuc', 'a', 'Ġand', 'ĠI', 'Ġhave', 'Ġplayed', 'Ġa', 'Ġlot']\n",
      "BERT    [101, 8667, 146, 112, 182, 16730, 1105, 146, 1138, 1307, 170, 1974, 102]\n",
      "ROBERTA [0, 31414, 38, 437, 7483, 102, 8, 38, 33, 702, 10, 319, 2]\n",
      "BERT    {'input_ids': tensor([[  101,  8667,   146,   112,   182, 16730,  1105,   146,  1138,  1307,\n",
      "           170,  1974,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ROBERTA {'input_ids': tensor([[    0, 31414,    38,   437,  7483,   102,     8,    38,    33,   702,\n",
      "            10,   319,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Tests\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "NEUTRAL  = 'neutral'\n",
    "POSITIVE = 'positive'\n",
    "NEGATIVE = 'negative'\n",
    "CONFLICT = 'conflict'\n",
    "ABSENT   = 'absent'\n",
    "l_vocab = { NEUTRAL  : 0,\n",
    "            POSITIVE : 1,\n",
    "            NEGATIVE : 2,\n",
    "            CONFLICT : 3,\n",
    "            ABSENT   : 4}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = l_vocab[ABSENT]\n",
    "print(a)\n",
    "b = torch.LongTensor([a])\n",
    "print(b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "þ = {'targets' : []}\n",
    "print(len(þ['targets']))\n",
    "þ['targets'].append(5)\n",
    "print(þ)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "þ = [{'targets': [['Appetizers', 'negative'], [['main dishes', 'negative']]]},\n",
    "     {'targets': [['view', 'negative']]},\n",
    "     {'targets': [['reservation', 'negative']]}]\n",
    "\n",
    "for elem in þ:\n",
    "    for term_pred in elem['targets']:\n",
    "        print(term_pred)\n",
    "        print(term_pred[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "path = \"/media/nemo/DATA/uni/nlp-hw2/model/model_b/BERT-2-SeqCls-model_TASK_B_both-to-both_term_epoch=0_step=2360_train_loss=1.74_macro_f1=17.54.ckpt\"\n",
    "new_path = \"/media/nemo/DATA/uni/nlp-hw2/model/model_b/BERT-2-SeqCls-model_TASK_B_both-to-both_term_epoch=0_step=2360_train_loss=1.74_macro_f1=17.54_CORRECT.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(path)\n",
    "new_checkpoint = OrderedDict()\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    if k == 'state_dict':\n",
    "        new_checkpoint[k] = OrderedDict()\n",
    "        for key, value in checkpoint['state_dict'].items():\n",
    "            new_key = key.replace('encoder', 'model.bert', 1)\n",
    "            new_checkpoint['state_dict'][new_key] = value\n",
    "            print(key, new_key)\n",
    "    else:\n",
    "        new_checkpoint[k] = v\n",
    "\n",
    "torch.save(new_checkpoint, new_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ROBERTA tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"roberta-base\")   # is cased\n",
    "model      = AutoModelWithLMHead.from_pretrained(\"roberta-base\")\n",
    "base_model = model.roberta "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = \"Hello my <mask> is Luca\"\n",
    "enc  = tokenizer.encode_plus(text)\n",
    "enc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\n",
    "out#.last_hidden_state.size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "model.classifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "for i in range(100):\n",
    "    print(progress[i % len(progress)], end=\"\\r\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = [1,1,1,1,1,1]\n",
    "for i in range(len(a)-1):\n",
    "    print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "raw_data = []\n",
    "with open('/media/nemo/DATA/uni/nlp-hw2/data/restaurants_train.json', 'r') as f:\n",
    "    raw_data.extend(json.load(f))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "c1 = Counter()\n",
    "c2 = Counter()\n",
    "c3 = Counter()\n",
    "c4 = Counter()\n",
    "zerouno = 0\n",
    "zerodue = 0\n",
    "zerotre = 0\n",
    "zeroqua = 0\n",
    "zerocin = 0\n",
    "\n",
    "unouno = 0\n",
    "unodue = 0\n",
    "unotre = 0\n",
    "unoqua = 0\n",
    "unocin = 0\n",
    "\n",
    "dueuno = 0\n",
    "duedue = 0\n",
    "duetre = 0\n",
    "duequa = 0\n",
    "duecin = 0\n",
    "\n",
    "treuno = 0\n",
    "tredue = 0\n",
    "tretre = 0\n",
    "trequa = 0\n",
    "trecin = 0\n",
    "\n",
    "quauno = 0\n",
    "quadue = 0\n",
    "quatre = 0\n",
    "quaqua = 0\n",
    "quacin = 0\n",
    "\n",
    "cinuno = 0\n",
    "cindue = 0\n",
    "cintre = 0\n",
    "cinqua = 0\n",
    "cincin = 0\n",
    "\n",
    "seiuno = 0\n",
    "seidue = 0\n",
    "seitre = 0\n",
    "seiqua = 0\n",
    "seicin = 0\n",
    "\n",
    "setuno = 0\n",
    "setdue = 0\n",
    "settre = 0\n",
    "setqua = 0\n",
    "setcin = 0\n",
    "\n",
    "ottuno = 0\n",
    "ottdue = 0\n",
    "otttre = 0\n",
    "ottqua = 0\n",
    "ottcin = 0\n",
    "\n",
    "novuno = 0\n",
    "novdue = 0\n",
    "novtre = 0\n",
    "novqua = 0\n",
    "novcin = 0\n",
    "\n",
    "for elem in raw_data:\n",
    "    categories = elem['categories']\n",
    "    c3[len(elem['categories'])] += 1\n",
    "    c4[len(elem['targets'])] += 1\n",
    "    \n",
    "    if len(elem['categories']) == 1    and len(elem['targets']) == 0:\n",
    "        zerouno += 1                  # no. sentences with zero targets and 1 category\n",
    "    elif len(elem['categories']) == 2  and len(elem['targets']) == 0:\n",
    "        zerodue += 1                  # no. sentences with zero targets and 1 category\n",
    "    elif len(elem['categories']) == 3  and len(elem['targets']) == 0:\n",
    "        zerotre += 1                  # no. sentences with zero targets and 1 category\n",
    "    elif len(elem['categories']) == 4  and len(elem['targets']) == 0:\n",
    "        zeroqua += 1                  # no. sentences with zero targets and 1 category\n",
    "    elif len(elem['categories']) == 5  and len(elem['targets']) == 0:\n",
    "        zerocin += 1 \n",
    "\n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 1:\n",
    "        unouno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 1:\n",
    "        unodue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 1:\n",
    "        unotre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 1:\n",
    "        unoqua += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 1:\n",
    "        unocin += 1\n",
    "\n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 2:\n",
    "        dueuno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 2:\n",
    "        duedue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 2:\n",
    "        duetre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 2:\n",
    "        duequa += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 2:\n",
    "        duecin += 1\n",
    "\n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 3:\n",
    "        treuno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 3:\n",
    "        tredue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 3:\n",
    "        tretre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 3:\n",
    "        trequa += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 3:\n",
    "        trecin += 1\n",
    "\n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 4:\n",
    "        quauno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 4:\n",
    "        quadue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 4:\n",
    "        quatre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 4:\n",
    "        quaqua += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 4:\n",
    "        quacin += 1\n",
    "    \n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 5:\n",
    "        cinuno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 5:\n",
    "        cindue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 5:\n",
    "        cintre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 5:\n",
    "        cinqua += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 5:\n",
    "        cincin += 1\n",
    "    \n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 6:\n",
    "        seiuno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 6:\n",
    "        seidue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 6:\n",
    "        seitre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 6:\n",
    "        seiqua += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 6:\n",
    "        seicin += 1\n",
    "\n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 7:\n",
    "        setuno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 7:\n",
    "        setdue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 7:\n",
    "        settre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 7:\n",
    "        setqua += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 7:\n",
    "        setcin += 1\n",
    "\n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 8:\n",
    "        ottuno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 8:\n",
    "        ottdue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 8:\n",
    "        otttre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 8:\n",
    "        ottqua += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 8:\n",
    "        ottcin += 1\n",
    "    \n",
    "    elif len(elem['categories']) == 1 and len(elem['targets']) == 9:\n",
    "        novuno += 1\n",
    "    elif len(elem['categories']) == 2 and len(elem['targets']) == 9:\n",
    "        novdue += 1\n",
    "    elif len(elem['categories']) == 3 and len(elem['targets']) == 9:\n",
    "        novtre += 1\n",
    "    elif len(elem['categories']) == 4 and len(elem['targets']) == 9:\n",
    "        novqua += 1\n",
    "    elif len(elem['categories']) == 5 and len(elem['targets']) == 9:\n",
    "        novcin += 1\n",
    "\n",
    "    for cat in categories:\n",
    "        c1[cat[0]] += 1\n",
    "        c2[cat[1]] += 1\n",
    "\n",
    "print(\"category terms\")\n",
    "pprint(c1)\n",
    "print(\"\\ncategory polarity\")\n",
    "pprint(c2)\n",
    "print(\"\\nnumero di frasi con x categorie\")\n",
    "pprint(c3)\n",
    "print(\"\\nnumero di frasi con x termini\")\n",
    "pprint(c4)\n",
    "print(\"\\n\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"numero di frasi con 0 termini e 1 categoria:\", zerouno)\n",
    "print(\"numero di frasi con 0 termini e 2 categorie:\", zerodue)\n",
    "print(\"numero di frasi con 0 termini e 3 categorie:\", zerotre)\n",
    "print(\"numero di frasi con 0 termini e 4 categorie:\", zeroqua)\n",
    "print(\"numero di frasi con 0 termini e 5 categorie:\", zerocin)\n",
    "print(\"\\n\")\n",
    "print(\"numero di frasi con 1 termini e 1 categoria:\", unouno)\n",
    "print(\"numero di frasi con 1 termini e 2 categorie:\", unodue)\n",
    "print(\"numero di frasi con 1 termini e 3 categorie:\", unotre)\n",
    "print(\"numero di frasi con 1 termini e 4 categorie:\", unoqua)\n",
    "print(\"numero di frasi con 1 termini e 5 categorie:\", unocin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"numero di frasi con 2 termini e 1 categoria:\", dueuno)\n",
    "print(\"numero di frasi con 2 termini e 2 categorie:\", duedue)\n",
    "print(\"numero di frasi con 2 termini e 3 categorie:\", duetre)\n",
    "print(\"numero di frasi con 2 termini e 4 categorie:\", duequa)\n",
    "print(\"numero di frasi con 2 termini e 5 categorie:\", duecin)\n",
    "print(\"\\n\")\n",
    "print(\"numero di frasi con 3 termini e 1 categoria:\", treuno)\n",
    "print(\"numero di frasi con 3 termini e 2 categorie:\", tredue)\n",
    "print(\"numero di frasi con 3 termini e 3 categorie:\", tretre)\n",
    "print(\"numero di frasi con 3 termini e 4 categorie:\", trequa)\n",
    "print(\"numero di frasi con 3 termini e 5 categorie:\", trecin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"numero di frasi con 4 termini e 1 categoria:\", quauno)\n",
    "print(\"numero di frasi con 4 termini e 2 categorie:\", quadue)\n",
    "print(\"numero di frasi con 4 termini e 3 categorie:\", quatre)\n",
    "print(\"numero di frasi con 4 termini e 4 categorie:\", quaqua)\n",
    "print(\"numero di frasi con 4 termini e 5 categorie:\", quacin)\n",
    "print(\"\\n\")\n",
    "print(\"numero di frasi con 5 termini e 1 categoria:\", cinuno)\n",
    "print(\"numero di frasi con 5 termini e 2 categorie:\", cindue)\n",
    "print(\"numero di frasi con 5 termini e 3 categorie:\", cintre)\n",
    "print(\"numero di frasi con 5 termini e 4 categorie:\", cinqua)\n",
    "print(\"numero di frasi con 5 termini e 5 categorie:\", cincin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"numero di frasi con 6 termini e 1 categoria:\", seiuno)\n",
    "print(\"numero di frasi con 6 termini e 2 categorie:\", seidue)\n",
    "print(\"numero di frasi con 6 termini e 3 categorie:\", seitre)\n",
    "print(\"numero di frasi con 6 termini e 4 categorie:\", seiqua)\n",
    "print(\"numero di frasi con 6 termini e 5 categorie:\", seicin)\n",
    "print(\"\\n\")\n",
    "print(\"numero di frasi con 7 termini e 1 categoria:\", setuno)\n",
    "print(\"numero di frasi con 7 termini e 2 categorie:\", setdue)\n",
    "print(\"numero di frasi con 7 termini e 3 categorie:\", settre)\n",
    "print(\"numero di frasi con 7 termini e 4 categorie:\", setqua)\n",
    "print(\"numero di frasi con 7 termini e 5 categorie:\", setcin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"numero di frasi con 8 termini e 1 categoria:\", ottuno)\n",
    "print(\"numero di frasi con 8 termini e 2 categorie:\", ottdue)\n",
    "print(\"numero di frasi con 8 termini e 3 categorie:\", otttre)\n",
    "print(\"numero di frasi con 8 termini e 4 categorie:\", ottqua)\n",
    "print(\"numero di frasi con 8 termini e 5 categorie:\", ottcin)\n",
    "print(\"\\n\")\n",
    "print(\"numero di frasi con 9 termini e 1 categoria:\", novuno)\n",
    "print(\"numero di frasi con 9 termini e 2 categorie:\", novdue)\n",
    "print(\"numero di frasi con 9 termini e 3 categorie:\", novtre)\n",
    "print(\"numero di frasi con 9 termini e 4 categorie:\", novqua)\n",
    "print(\"numero di frasi con 9 termini e 5 categorie:\", novcin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t = [[0.7371, 0.7371]]\n",
    "import torch\n",
    "i = torch.argmax(torch.Tensor(t)).item()\n",
    "pred = [0, 0]\n",
    "pred[i] = 1\n",
    "pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out = [0, 0, 0, 0, 0]\n",
    "print(1 in out)\n",
    "import torch\n",
    "out[torch.argmax(torch.Tensor([0,0,10,0,0])).item()] = 1\n",
    "print(1 in out)\n",
    "out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test uncased A&B\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text_sample = \"Appetizers are somewhere around $7 each and the main dishes are between $11 and $16.\"\n",
    "net_outputs = {\"targets\": [[\"appetizers\", \"neutral\"], [\"main dishes\", \"neutral\"]]}\n",
    "\n",
    "text = \"It seemed to be a very nice laptop except I was not able to load my Garmin GPS software or Microsoft Office 2003.\"\n",
    "text = \"With the macbook pro it comes with freesecuritysoftware to protect it from viruses and other intrusive things from downloads and internet surfing or emails.\"\n",
    "print(tokenizer.tokenize(text))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_sample = \"There are several programs for school or office use (Pages, Numbers, Keynote, etc.), music (Garageband), photo management (Photo Booth, iPhoto), video-editing or movie-making (iMovie), etc.\"\n",
    "net_outputs = {'targets': [['iphoto'], ['garageband'], ['music'], ['imovie'], ['photo booth'], ['programs'], ['photo management']]}\n",
    "\n",
    "listed_text = tokenizer.tokenize(text_sample, add_special_tokens=False)\n",
    "print(net_outputs)\n",
    "print(listed_text)\n",
    "print()\n",
    "\n",
    "for pred in net_outputs['targets']:\n",
    "    term = pred[0]\n",
    "    print(term, \"in text:\", term in text_sample)\n",
    "    \n",
    "    if term not in text_sample:\n",
    "        listed_term = tokenizer.tokenize(term, add_special_tokens=False)\n",
    "        print(listed_term)\n",
    "\n",
    "        for i in range(len(listed_text)):\n",
    "            word = listed_text[i]\n",
    "            curr = listed_term[0]\n",
    "\n",
    "            if curr == word.lower():\n",
    "                if len(listed_text) >= i + len(listed_term):\n",
    "                    aux1 = ''\n",
    "                    aux2 = ''\n",
    "\n",
    "                    for j in range(len(listed_term)):\n",
    "                        tmp1 = listed_text[i+j]\n",
    "                        if len(tmp1) > 2 and tmp1[0] == tmp1[1] == '#':\n",
    "                            tmp1 = tmp1[2:]               # ignore the starting `##`\n",
    "                            aux1 = aux1.strip(' ')      # remove the space before (if any)\n",
    "                        aux1 += tmp1 + ' '\n",
    "\n",
    "                        tmp2 = listed_term[j]\n",
    "                        if len(tmp2) > 2 and tmp2[0] == tmp2[1] == '#':\n",
    "                            tmp2 = tmp2[2:]               # ignore the starting `##`\n",
    "                            aux2 = aux2.strip(' ')      # remove the space before (if any)\n",
    "                        aux2 += tmp2  + ' '\n",
    "                    \n",
    "                    print(\"g\", aux1)\n",
    "                    print(\"p\", aux2)\n",
    "                    print()\n",
    "                    \n",
    "                    if aux1.strip().lower() == aux2.strip():\n",
    "                        pred[0] = aux1.strip()\n",
    "                        print(\">> Changed to:\", pred[0])    \n",
    "\n",
    "\n",
    "\n",
    "print(net_outputs)\n",
    "                \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "def tokenize_line(line, pattern='\\s'):\n",
    "    line = re.sub('[\\.,:;!@#$\\(\\)\\-&\\\\<>]', '-', line)\n",
    "    return [word.strip('-') for word in re.split(pattern, line) if word]\n",
    "\n",
    "def tokenize_line_2(line,pattern='\\s'):\n",
    "    line = re.sub(' ', '-', line)\n",
    "    return [word.strip('-') for word in re.split(pattern, line) if word]\n",
    "\n",
    "text_sample = \"There are several programs for school or office use (Pages, Numbers, Keynote, etc.), music (Garageband), photo management (Photo Booth, iPhoto), video-editing or movie-making (iMovie), etc.\"\n",
    "net_outputs = {'targets': [['iphoto'], ['garageband'], ['music'], ['imovie'], ['photo booth'], ['programs'], ['photo management']]}\n",
    "\n",
    "text_sample = \"But when I received my replacement, I made BOTH recovery DVDs (4), and a driver/application DVD.\"\n",
    "net_outputs = {'targets': [[\"driver/application dvd\"], [\"recovery dvds\"]]}\n",
    "\n",
    "#text_sample = \"/ awesome cooling system/ much better grafics card (ATI 5870) / 8GB RAM/ LED backlit screen...\"\n",
    "#net_outputs = {'targets': [[\"8gb ram\"]]}\n",
    "\n",
    "#text_sample = \"I love the multi-touch trackpad.\"\n",
    "#net_outputs = {'targets': [[\"multi touch\"]]}\n",
    "\n",
    "text_sample = \"the headphone and mic jack are in front of touch-pad making the touch-pad hard to use when using headphones/mic, not to mention the laptop was designed for right handed person.\"\n",
    "net_outputs = {'targets': [[\"touch - pad\"], ['mic jack'], ['headphone'], ['headphones'], ['mic']]}\n",
    "\n",
    "listed_text = tokenize_line(text_sample)\n",
    "print(net_outputs)\n",
    "print(listed_text)\n",
    "\n",
    "for pred in net_outputs['targets']:\n",
    "    term = pred[0]\n",
    "    print(term, \"in text:\", term in text_sample)\n",
    "    \n",
    "    if term not in text_sample:\n",
    "        if ' - ' in term:\n",
    "            term = term.replace(' - ', '-')\n",
    "        listed_term = tokenize_line(term)\n",
    "        print(\"tokenized term:\", listed_term)\n",
    "\n",
    "        for i in range(len(listed_text)):\n",
    "            word = listed_text[i]\n",
    "            curr = listed_term[0]\n",
    "    \n",
    "            if curr == word.lower():\n",
    "                if len(listed_text) >= i + len(listed_term):\n",
    "                    aux1 = ''\n",
    "                    aux2 = ''\n",
    "                    for j in range(len(listed_term)):\n",
    "                        #if listed_text[i+j].lower() == listed_term[j]:\n",
    "                        aux1 += listed_text[i+j] + ' '\n",
    "                        aux2 += listed_term[j]   + ' '   \n",
    "                        #elif listed_text[i+j] == '-' and listed_term == '':\n",
    "                        #    aux1 = aux1.strip() + '-'\n",
    "                        #    aux2 = aux2.strip() + '-'\n",
    "                    \n",
    "                    if aux1.strip().lower() == aux2.strip():\n",
    "                        pred[0] = aux1.strip()\n",
    "                        print(\"1 >> Changed to:\", pred[0])\n",
    "                    \n",
    "                    aux2 = aux2.strip(' ') + '/'    # one case I can't write in regex: '8GB RAM/'\n",
    "                    if aux1.strip().lower() == aux2.strip():\n",
    "                        pred[0] = aux1.strip().strip('/')\n",
    "                        print(\"2 >> Changed to:\", pred[0])\n",
    "        '''\n",
    "        if pred[0] not in text_sample:\n",
    "            if ' - ' in pred[0]:\n",
    "                term = term.replace(' - ', '-')\n",
    "                print(term)\n",
    "            listed_term = tokenize_line_2(term)\n",
    "            print(\"tokenized term 2\", listed_term)\n",
    "\n",
    "            for i in range(len(listed_text)):\n",
    "                word = listed_text[i]\n",
    "                curr = listed_term[0]\n",
    "        \n",
    "                if curr == word.lower():\n",
    "                    if len(listed_text) >= i + len(listed_term):\n",
    "                        aux1 = ''\n",
    "                        aux2 = ''\n",
    "                        for j in range(len(listed_term)):\n",
    "                            aux1 += listed_text[i+j] + ' '\n",
    "                            aux2 += listed_term[j]   + ' '   \n",
    "\n",
    "                        if aux1.strip().lower() == aux2.strip():\n",
    "                            pred[0] = aux1.strip()\n",
    "                            print(\"3 >> Changed to:\", pred[0])\n",
    "                        \n",
    "                        aux2 = aux2.strip(' ') + '/'    # one case I can't write in regex: '8GB RAM/'\n",
    "                        if aux1.strip().lower() == aux2.strip():\n",
    "                            pred[0] = aux1.strip().strip('/')\n",
    "                            print(\"4 >> Changed to:\", pred[0])\n",
    "        '''\n",
    " \n",
    "print(net_outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stopwords tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir='./nltk_data')\n",
    "nltk.data.path.append('./nltk_data')\n",
    "stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: nltk in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied: click in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from nltk) (2021.7.6)\n",
      "Requirement already satisfied: tqdm in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from click->nltk) (4.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "sent = 'From the moment you enter till the moment you walk out the friendly and helpful staff was was just Fantastic.'\n",
    "for word in stopset:\n",
    "    if word in sent.split():\n",
    "        sent = sent.replace(word, '')\n",
    "print(sent)\n",
    "tokenizer.tokenize(sent)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From  moment  enter till  moment  walk   friendly  helpful staff    Fantastic.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['from',\n",
       " 'moment',\n",
       " 'enter',\n",
       " 'till',\n",
       " 'moment',\n",
       " 'walk',\n",
       " 'friendly',\n",
       " 'helpful',\n",
       " 'staff',\n",
       " 'fantastic',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install vaderSentiment"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 3.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from requests->vaderSentiment) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages (from requests->vaderSentiment) (2021.5.30)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# import SentimentIntensityAnalyzer class\n",
    "# from vaderSentiment.vaderSentiment module.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    " \n",
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_scores(sentence):\n",
    " \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    print(f\"Negative: {sentiment_dict['neg']*100:.2f}%\")\n",
    "    print(f\"Positive: {sentiment_dict['neu']*100:.2f}%\")\n",
    "    print(f\"Neutral : {sentiment_dict['pos']*100:.2f}%\")\n",
    "    print(f\"Compound: {sentiment_dict['compound']*100:.2f}%\")\n",
    " \n",
    "    print(\">>\", end = \" \")\n",
    " \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    " \n",
    " \n",
    "   \n",
    "# Driver code\n",
    "if __name__ == \"__main__\" :\n",
    " \n",
    "    sentence = \"I love you\"\n",
    " \n",
    "    # function calling\n",
    "    sentiment_scores(sentence)\n",
    " \n",
    "    sentence = \"study is going on as usual\"\n",
    "    sentiment_scores(sentence)\n",
    " \n",
    "    sentence = \"I am vey sad today.\"\n",
    "    sentiment_scores(sentence)\n",
    "\n",
    "    sentence = \"My wife and I always enjoy the young, not always well trained but nevertheless friendly, staff, all of whom have a story.\"\n",
    "    sentiment_scores(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Negative: 0.00%\n",
      "Positive: 32.30%\n",
      "Neutral : 67.70%\n",
      "Compound: 63.69%\n",
      ">> Positive\n",
      "Negative: 0.00%\n",
      "Positive: 100.00%\n",
      "Neutral : 0.00%\n",
      "Compound: 0.00%\n",
      ">> Neutral\n",
      "Negative: 43.70%\n",
      "Positive: 56.30%\n",
      "Neutral : 0.00%\n",
      "Compound: -47.67%\n",
      ">> Negative\n",
      "Negative: 5.20%\n",
      "Positive: 70.90%\n",
      "Neutral : 23.90%\n",
      "Compound: 71.78%\n",
      ">> Positive\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "a = [int(elem) for elem in '-1'.split(':')]\n",
    "\n",
    "'concat' in 'concat-max'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/nemo/.local/share/jupyter/runtime/kernel-c425b673-e956-48a4-be62-84d559b0a21c.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/nemo/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "sent = \"I love this PIZZA!\"\n",
    "shit = \"shit\"\n",
    "encoded   = tokenizer.encode(sent, add_special_tokens=True)\n",
    "decoded   = tokenizer.decode(encoded[1:-1])\n",
    "aux = tokenizer.tokenize(sent)\n",
    "aux.insert(0, '<s>')\n",
    "aux.insert(len(aux), '</s>')\n",
    "#print(encoded, len(encoded))\n",
    "#print(decoded)\n",
    "print(sent)\n",
    "print(aux, len(aux))\n",
    "\n",
    "þ = ''\n",
    "for i in range(len(aux)):\n",
    "    if aux[i] == '<s>' or aux[i] == '</s>':\n",
    "        continue\n",
    "    if aux[i] == '<pad>':\n",
    "        break\n",
    "    if aux[i] == '<unk>':\n",
    "        þ += ' ' + '<unk>'\n",
    "\n",
    "    if aux[i][0] == 'Ġ':\n",
    "        þ += ' ' + aux[i][1:]\n",
    "    else:\n",
    "        þ += aux[i]\n",
    "\n",
    "print(þ)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I love this PIZZA!\n",
      "['<s>', 'I', 'Ġlove', 'Ġthis', 'ĠP', 'IZ', 'ZA', '!', '</s>'] 9\n",
      "I love this PIZZA!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "sent = 'I Love this PIZZA!'\n",
    "inputs = tokenizer( sent,\n",
    "                    truncation=True,\n",
    "                    padding='longest',\n",
    "                    return_tensors='pt')\n",
    "\n",
    "# build the labels\n",
    "import numpy as np\n",
    "terms_vectors = np.full_like(inputs['input_ids'], dtype=int,\n",
    "                            fill_value=0)\n",
    "current = inputs['input_ids'][0]\n",
    "encoded_targets = tokenizer.encode(' PIZZA', add_special_tokens=False)\n",
    "\n",
    "print(\"text    \", tokenizer.tokenize(sent))\n",
    "print(\"text ids\", current.numpy())\n",
    "print(\"targets \", encoded_targets)\n",
    "print(\"label   \", terms_vectors)\n",
    "\n",
    "for i in range(len(inputs['input_ids'][0])):\n",
    "    if current[i] == encoded_targets[0]:\n",
    "        term_len = len(encoded_targets)\n",
    "        if len(current) - i >= term_len:    # prevents IndexOutOfError for prefix matches\n",
    "            aux1 = []\n",
    "            aux2 = []\n",
    "            for j in range(term_len):\n",
    "                aux1.append(current[i+j].item())\n",
    "                aux2.append(encoded_targets[j])\n",
    "            if aux1 == aux2:                            # I check all the sequence long 'term_len'\n",
    "                for j in range(term_len):\n",
    "                    terms_vectors[0][i+j] = 2\n",
    "                terms_vectors[0][i]       = 1\n",
    "                continue\n",
    "print(\"label   \", terms_vectors)\n",
    "\n",
    "for i in range(len(inputs['input_ids'][0])):\n",
    "    if current[i] == 1:\n",
    "        break\n",
    "    elif current[i] in [0, 2]:\n",
    "        continue\n",
    "    else:\n",
    "        if terms_vectors[0][i] == 0:\n",
    "            terms_vectors[0][i] = 3\n",
    "print(\"label   \", terms_vectors)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "text     ['I', 'ĠLove', 'Ġthis', 'ĠP', 'IZ', 'ZA', '!']\n",
      "text ids [    0   100  3437    42   221 17045 22447   328     2]\n",
      "targets  [221, 17045, 22447]\n",
      "label    [[0 0 0 0 0 0 0 0 0]]\n",
      "label    [[0 3 3 3 1 2 2 3 0]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}